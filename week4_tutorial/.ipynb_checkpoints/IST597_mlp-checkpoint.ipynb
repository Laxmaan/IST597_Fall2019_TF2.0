{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pt6zvw6zd2YO"
   },
   "source": [
    "#IST597- softmax model(First step towards building your neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_qqH0TCCbx7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68LhmrMICdic"
   },
   "source": [
    "Load tensorflow and mnist data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVwpJUmfCpds"
   },
   "outputs": [],
   "source": [
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2krCqRcCq65"
   },
   "source": [
    "Check whether or not you're working in eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1AKNWVkCzuJ"
   },
   "outputs": [],
   "source": [
    "W = tf.get_variable(name=\"W\", shape=(784, 10))\n",
    "b = tf.get_variable(name=\"b\", shape=(10, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZaI6Xy3C16c"
   },
   "source": [
    "Create your tensorflow variables\n",
    "Create Weight and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIg-qgZSDIT9"
   },
   "outputs": [],
   "source": [
    "def softmax_model(image_batch):\n",
    "    model_output = tf.nn.softmax(tf.matmul(image_batch, W) + b)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlEOU3mdDKr8"
   },
   "source": [
    "We have created the softmax_model\n",
    "output = F(X.W+b)\n",
    "where X is input , W is weight and b is biases.\n",
    "F is non-linear function , softmax in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S09GtsH3D241"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(model_output, label_batch):\n",
    "    loss = tf.reduce_mean(\n",
    "        -tf.reduce_sum(label_batch * tf.log(model_output),\n",
    "        reduction_indices=[1]))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mezQBqoeELOr"
   },
   "source": [
    "Define your loss: In this case we would be using cross-entropy(NLL)\n",
    "Cross-entropy loss:- log loss is responsible for measuring the performance of a model which gives value between 0 and 1. A perfect model would have a log loss of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMRl00yAGdpn"
   },
   "outputs": [],
   "source": [
    "@tfe.implicit_value_and_gradients\n",
    "def cal_gradient(image_batch, label_batch):\n",
    "    return cross_entropy(softmax_model(image_batch), label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJouUWYIGpwm"
   },
   "source": [
    "This would returns a function which differentiates loss function with respect to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9tdA5t6W_Mb4",
    "outputId": "4e75101c-7952-4fa4-aa03-8c723114122d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step: 0  loss: 2.4309253692626953\n",
      "step: 100  loss: 1.6556241512298584\n",
      "step: 200  loss: 1.2155452966690063\n",
      "step: 300  loss: 1.0475245714187622\n",
      "step: 400  loss: 1.0862095355987549\n",
      "step: 500  loss: 0.7784752249717712\n",
      "step: 600  loss: 0.8664791584014893\n",
      "step: 700  loss: 0.7498409748077393\n",
      "step: 800  loss: 0.698955774307251\n",
      "step: 0  loss: 0.6215534210205078\n",
      "step: 100  loss: 0.643366813659668\n",
      "step: 200  loss: 0.5203976631164551\n",
      "step: 300  loss: 0.6068369150161743\n",
      "step: 400  loss: 0.7616375088691711\n",
      "step: 500  loss: 0.5042001008987427\n",
      "step: 600  loss: 0.6564253568649292\n",
      "step: 700  loss: 0.5584914088249207\n",
      "step: 800  loss: 0.5750884413719177\n",
      "step: 0  loss: 0.47606679797172546\n",
      "step: 100  loss: 0.5176450610160828\n",
      "step: 200  loss: 0.4088672995567322\n",
      "step: 300  loss: 0.5149017572402954\n",
      "step: 400  loss: 0.6724463105201721\n",
      "step: 500  loss: 0.4298582673072815\n",
      "step: 600  loss: 0.584412693977356\n",
      "step: 700  loss: 0.4898722171783447\n",
      "step: 800  loss: 0.5216456651687622\n",
      "step: 0  loss: 0.4172459840774536\n",
      "step: 100  loss: 0.46674051880836487\n",
      "step: 200  loss: 0.3576718270778656\n",
      "step: 300  loss: 0.47118520736694336\n",
      "step: 400  loss: 0.6281046271324158\n",
      "step: 500  loss: 0.39349788427352905\n",
      "step: 600  loss: 0.5446990728378296\n",
      "step: 700  loss: 0.45289987325668335\n",
      "step: 800  loss: 0.48751479387283325\n",
      "step: 0  loss: 0.38431769609451294\n",
      "step: 100  loss: 0.43784481287002563\n",
      "step: 200  loss: 0.3266371488571167\n",
      "step: 300  loss: 0.44390714168548584\n",
      "step: 400  loss: 0.6007729768753052\n",
      "step: 500  loss: 0.3711305856704712\n",
      "step: 600  loss: 0.5183553099632263\n",
      "step: 700  loss: 0.42935237288475037\n",
      "step: 800  loss: 0.4623306393623352\n",
      "step: 0  loss: 0.36290255188941956\n",
      "step: 100  loss: 0.4184116721153259\n",
      "step: 200  loss: 0.30524325370788574\n",
      "step: 300  loss: 0.42441099882125854\n",
      "step: 400  loss: 0.581855058670044\n",
      "step: 500  loss: 0.3556104302406311\n",
      "step: 600  loss: 0.4991150200366974\n",
      "step: 700  loss: 0.4128991961479187\n",
      "step: 800  loss: 0.4423584043979645\n",
      "step: 0  loss: 0.34770071506500244\n",
      "step: 100  loss: 0.4039914906024933\n",
      "step: 200  loss: 0.28935688734054565\n",
      "step: 300  loss: 0.40934592485427856\n",
      "step: 400  loss: 0.5677635669708252\n",
      "step: 500  loss: 0.3440224528312683\n",
      "step: 600  loss: 0.4842134714126587\n",
      "step: 700  loss: 0.4006926417350769\n",
      "step: 800  loss: 0.4258308708667755\n",
      "step: 0  loss: 0.33626943826675415\n",
      "step: 100  loss: 0.39260417222976685\n",
      "step: 200  loss: 0.27697357535362244\n",
      "step: 300  loss: 0.39712393283843994\n",
      "step: 400  loss: 0.5567216873168945\n",
      "step: 500  loss: 0.3349366784095764\n",
      "step: 600  loss: 0.4722058176994324\n",
      "step: 700  loss: 0.3912419080734253\n",
      "step: 800  loss: 0.4117637276649475\n",
      "step: 0  loss: 0.3273158669471741\n",
      "step: 100  loss: 0.38322532176971436\n",
      "step: 200  loss: 0.26698529720306396\n",
      "step: 300  loss: 0.3868834376335144\n",
      "step: 400  loss: 0.547744631767273\n",
      "step: 500  loss: 0.32756030559539795\n",
      "step: 600  loss: 0.4622482657432556\n",
      "step: 700  loss: 0.38368332386016846\n",
      "step: 800  loss: 0.3995473384857178\n",
      "step: 0  loss: 0.32008713483810425\n",
      "step: 100  loss: 0.37526506185531616\n",
      "step: 200  loss: 0.25872135162353516\n",
      "step: 300  loss: 0.3781081438064575\n",
      "step: 400  loss: 0.5402401685714722\n",
      "step: 500  loss: 0.3214142322540283\n",
      "step: 600  loss: 0.4538081884384155\n",
      "step: 700  loss: 0.37748056650161743\n",
      "step: 800  loss: 0.3887752890586853\n",
      "test accuracy = 0.90829998254776\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "batch_size=64\n",
    "epoch_n=10\n",
    "data = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "           .shuffle(buffer_size=1000)\\\n",
    "           .batch(batch_size=64)\\\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "  for step, (image_batch, label_batch) in enumerate(tfe.Iterator(train_ds)):\n",
    "      loss, grads_and_vars = cal_gradient(image_batch, label_batch)\n",
    "      optimizer.apply_gradients(grads_and_vars)\n",
    "      if(step%100 == 0):\n",
    "        print(\"step: {}  loss: {}\".format(step, loss.numpy()))\n",
    "\n",
    "model_test_output = softmax_model(data.test.images)\n",
    "model_test_label = data.test.labels\n",
    "correct_prediction = tf.equal(tf.argmax(model_test_output, 1), tf.argmax(model_test_label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"test accuracy = {}\".format(accuracy.numpy()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "IST597_mlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
